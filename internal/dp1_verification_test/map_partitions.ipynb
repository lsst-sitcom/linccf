{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b790eee",
   "metadata": {},
   "source": [
    "# map_partitions\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "  * how to apply a user-defined function using distributed computing across an entire LSDB catalog's partitions\n",
    "  * when, why, and how to supply required Dask \"meta\" when it cannot be inferred from your user-defined function\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LSDB/HATS catalogs are organized into partitions, and this number of partitions is reported as `npartitions=` in the header, whenever printing a catalog.  Each partition corresponds to a single pixel, and they have been sized to have approximately the same number of rows in each partition, in order to enable efficient parallel computation.\n",
    "\n",
    "The `map_partitions` method provides a means for users to execute their own analysis functions on each *partition* of the catalog data. The data will be passed to your function as a Pandas DataFrame (`pd.DataFrame`) object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbecd41",
   "metadata": {},
   "source": [
    "## 1. Open a catalog\n",
    "\n",
    "For this example, we will use a small box of Data Preview 1, and only specify columns of interest. This limits the overall memory requirements of the pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2225843f",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. nbinfo::\n",
    "    Additional Help \n",
    "    \n",
    "    For additional information on dask client creation, please refer to the \n",
    "    `official Dask documentation <https://distributed.dask.org/en/latest/client.html>`__ \n",
    "    and our :doc:`Dask cluster configuration </tutorials/dask-cluster-tips>` page for LSDB-specific tips. \n",
    "    Note that dask also provides its own `best practices <https://docs.dask.org/en/stable/best-practices.html>`__, which may also be useful to consult.\n",
    "    \n",
    "    For tips on accessing remote data, see our :doc:`Accessing remote data tutorial </tutorials/remote_data>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff803be2-c4c9-4bf8-8fe0-5b7e896d6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask puts out more advisory logging than we care for in this tutorial.\n",
    "# It takes some doing to quiet all of it, but this recipe works.\n",
    "\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"logging.distributed\": \"critical\"})\n",
    "\n",
    "import logging\n",
    "\n",
    "# This also has to be done, for the above to be effective\n",
    "logger = logging.getLogger(\"distributed\")\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Finally, suppress the specific warning about Dask dashboard port usage\n",
    "warnings.filterwarnings(\"ignore\", message=\"Port 8787 is already in use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21bad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb\n",
    "\n",
    "dp1 = lsdb.open_catalog(\"https://data.lsdb.io/hats/mock_dp1/object_collection\",\n",
    "                        search_filter=lsdb.BoxSearch(ra=[51.,53.], dec=[-30.,-28.5]),\n",
    "                       columns=[\"objectId\", \"coord_ra\", \"coord_dec\", \"g_psfMag\", \"i_psfMag\"])\n",
    "dp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the number of partitions programmatically this way.\n",
    "# This can be valuable when you want to choose the optimal number\n",
    "# of workers to process the partitions.\n",
    "dp1.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also access the individual HealpixPixel objects for\n",
    "# each partition, this way, inspecting their order and pixel,\n",
    "# if desired.\n",
    "px = dp1.get_healpix_pixels()[0]\n",
    "px.order, px.pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d0560",
   "metadata": {},
   "source": [
    "## 2. Generating New Columns\n",
    "\n",
    "Since the partition's `pd.DataFrame` is passed in to your custom function, you can augment it with new columns based on the existing columns, in ordinary Pandas style.\n",
    "\n",
    "### 2.1 What you can map\n",
    "\n",
    "The trick is understanding what kind of custom function you can pass to `.map_partitions`.  Your function is going to receive a Pandas DataFrame as its first parameter.  Other parameters can be passed in as keyword arguments to `.map_partitions`, as you'll see later on.  For now, we'll use a function that takes in one partition and produces a result that has the same shape.\n",
    "\n",
    "Because the catalog is loaded lazily, `.map_partitions` also returns a lazy, or unevaluated, result.  You can see the results the same way you can realize the original catalog, by any of these means:\n",
    "  * calling `.compute()` to produce a `pd.DataFrame` in memory;\n",
    "  * calling `.to_hats()` to serialize it to disk as a HATS-format file;\n",
    "  * calling `.head()` to see the first few rows, `.tail()` to see the last few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_minus_i(df, pixel):\n",
    "    df[\"psf_g_minus_i\"] = df[\"g_psfMag\"] - df[\"i_psfMag\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "unrealized = dp1.map_partitions(g_minus_i, include_pixel=True)\n",
    "unrealized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b439ab-5845-4090-8350-109e59775d5e",
   "metadata": {},
   "source": [
    "Taking a quick peek to see whether our function works correctly, and if the results in our new column are about what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884233a7-b6ef-47d7-aa38-30a48d19f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_5 = unrealized.head(5)\n",
    "head_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fa7e9-2fc3-4e26-89aa-cf050493e363",
   "metadata": {},
   "source": [
    "Looks good! Now on to computing the complete result.\n",
    "\n",
    "This unrealized result has a top-level property indicating how many partitions it has.  We can use this to choose our number of workers directly.\n",
    "\n",
    "However, it's a good idea to bound the number of workers, in case the number of partitions is larger than we expect (or we move this code fragment elsewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dask.distributed import Client\n",
    "\n",
    "npartitions = dp1.npartitions\n",
    "\n",
    "# Create a client which will be implicitly used until we make a new one\n",
    "client = Client(n_workers=min(2, npartitions), memory_limit=\"auto\")\n",
    "\n",
    "result = unrealized.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb134afe",
   "metadata": {},
   "source": [
    "No reduction step is needed here since the operation is not a reducing operation.\n",
    "There are as many rows in the new output as there were in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa390b4-cf26-4d69-aaeb-525979f79bc7",
   "metadata": {},
   "source": [
    "## 3. Functions that reduce\n",
    "\n",
    "The above works when your output rows are the same as your input rows.  When you're doing a reducing operation (such as calculating statistics), the process changes a little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ae459",
   "metadata": {},
   "source": [
    "### 3.1. Your function's parameters\n",
    "\n",
    "Again, your first input parameter is a `pd.DataFrame` that is one partition of the catalog, and the return value of your function needs to be the same, even if your result has only a single row.\n",
    "\n",
    "If you want to know the HEALPix number of the partition, calling `.map_partitions` with `include_pixel=True` will pass that as the second parameter to your function.  We'll do this in this example, for demonstration purposes, though it isn't strictly necessary to this task.\n",
    "\n",
    "If you have any other parameters that your function requires, take them as keyword arguments, and you can pass their values in as such, when calling `.map_partitions`.  Our example will do this, too, taking a `target_column=` argument.\n",
    "\n",
    "### 3.2. What you get back\n",
    "\n",
    "The operation we're going to do here is a reducing operation (min and max), and it will be run on each partition, reducing the many rows in each partition to a single value.  This means that the output of `.map_partitions` in this case will contain *one row per partition*.  Thus, you will need to do additional reduction on this output in order to get a single final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this function must work correctly when given an empty DataFrame\n",
    "# as an input, too; if not, you're obliged to provide \"meta\", that is,\n",
    "# information about output type and shape.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_stats(df, pixel, target_column=\"\"):\n",
    "    c = df[target_column]\n",
    "    min_val = c.min()\n",
    "    max_val = c.max()\n",
    "    mean_val = c.mean()\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"pixel\": pixel,\n",
    "                f\"{target_column}_min\": min_val,\n",
    "                f\"{target_column}_max\": max_val,\n",
    "                f\"{target_column}_mean\": mean_val,\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea19896-60e7-41c7-9886-29bcb2e91543",
   "metadata": {},
   "source": [
    "### 3.3 When You Need `meta=`\n",
    "\n",
    "The above definition of `find_stats` works even with an empty `pd.DataFrame` argument because `.mean()` is written to handle zero-row inputs without errors.\n",
    "\n",
    "But your own custom function might not. As a trivial example, suppose you implemented the arithmetic mean yourself, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bd821-ed54-49a7-8833-7b842077ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of the function will NOT work with map_partitions as is, because\n",
    "# of the attempt to divide by `c.count()`, which will be zero for an empty input.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_stats_needs_meta(df, pixel, target_column=\"\"):\n",
    "    c = df[target_column]\n",
    "    min_val = c.min()\n",
    "    max_val = c.max()\n",
    "    # WARNING! c.count() == 0 when passed an empty DataFrame.\n",
    "    # But meta= will come to the rescue.\n",
    "    mean_val = c.sum() / c.count()\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"pixel\": pixel,\n",
    "                f\"{target_column}_min\": min_val,\n",
    "                f\"{target_column}_max\": max_val,\n",
    "                f\"{target_column}_mean\": mean_val,\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c52776-2a83-49a6-97a2-1b8efc21f3fc",
   "metadata": {},
   "source": [
    "In the above case, then, you need to indicate to Dask what type the output will be.\n",
    "\n",
    "What Dask needs to know are the column names and their order, and so the below definition works, even though the types of the columns aren't indicated. (The type of `\"pixel\"` will default to `float64`, which is wrong, but doesn't matter in this case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d89aa-e24a-4858-8ec0-e0a54a6db1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_meta = pd.DataFrame(\n",
    "    {\n",
    "        \"pixel\": [],\n",
    "        \"g_psfMag_min\": [],\n",
    "        \"g_psfMag_max\": [],\n",
    "        \"g_psfMag_mean\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee5f86-8b4e-4bfd-a39b-30332b83071b",
   "metadata": {},
   "source": [
    "Here's another definition of `output_meta` that works equally well, and has the advantage that the way the `pd.DataFrame` is initialized is the same form as the return value in the custom function. The type of `\"pixel\"` will now be more correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bcbd9-a28c-4ada-ac58-76513f20ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_meta = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"pixel\": lsdb.HealpixPixel(0, 0),\n",
    "            \"g_psfMag_min\": 0.0,\n",
    "            \"g_psfMag_max\": 0.0,\n",
    "            \"g_psfMag_mean\": 0.0,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f497d9-8b44-4299-8831-5fff6abfdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_meta[\"pixel\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790832fe-e8cc-43e6-985a-82292e763961",
   "metadata": {},
   "source": [
    "although `pd.DataFrame` only cares that it is an \"Object\", as we can see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c00bfb-8aec-40c8-a79a-784b15c03dde",
   "metadata": {},
   "source": [
    "These can be complicated and error-prone to construct, and small mistakes can create confusing errors that show up late in the computation.\n",
    "\n",
    "To help with these difficulties, Dask does provide a `make_meta` function. If you can pass it a single valid row from your catalog data (that's what `head_5.head(1)` will do) which will work with your custom function, `make_meta` will generate the meta for it.\n",
    "\n",
    "**NOTE:** It's much faster here to use the computed data from last time (`head_5`) than trying to use `dp1.head(1)`. The latter will still give the right answer, but will be much slower, since it obliges Dask to re-execute the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264ec2c-4b38-4b76-be9f-da6ef2dee5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dask.dataframe.utils import make_meta\n",
    "\n",
    "output_meta = make_meta(\n",
    "    find_stats_needs_meta(head_5.head(1), dp1.get_healpix_pixels()[0], target_column=\"g_psfMag\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323fc0f-7cef-40f1-a37f-2fa7fb9c38df",
   "metadata": {},
   "source": [
    "Passing a correct `meta=` to `.map_partitions` will allow Dask to skip the process of sending your function an empty `pd.DataFrame`, and so, in our case of `find_stats_needs_meta` (where we depend on a non-zero `c.count()`), it will succeed without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unrealized = dp1.map_partitions(\n",
    "    find_stats_needs_meta,\n",
    "    include_pixel=True,\n",
    "    # Keyword arguments after 'include_pixel=' are passed to your function.\n",
    "    target_column=\"g_psfMag\",\n",
    "    # Here we give Dask the hint it needs to avoid giving us an empty frame\n",
    "    meta=output_meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = unrealized.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc892c79-0217-41f5-b5f0-2b04adb521e4",
   "metadata": {},
   "source": [
    "The objects in the 'pixel' column are the same type as from `get_healpix_pixels()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67458c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result[\"pixel\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e7ae-b726-45f8-85f1-585ce9844450",
   "metadata": {},
   "source": [
    "Because the result is one row per partition, we need additional reduction to get our single answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"g_psfMag_min\"].min(), result[\"g_psfMag_max\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca02ea-4d18-4eaa-b58f-6406b456eb9c",
   "metadata": {},
   "source": [
    "What about searching not only the four partitions from our cone search, but the whole catalog?  All that changes is the number of partitions.\n",
    "\n",
    "**NOTE** that since we are now using the `find_stats` that doesn't need `meta=`, we don't need to provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1_all = lsdb.open_catalog(\"https://data.lsdb.io/hats/mock_dp1/object_collection\",\n",
    "                           columns=[\"objectId\", \"coord_ra\", \"coord_dec\", \"g_psfMag\", \"i_psfMag\"])\n",
    "unrealized = dp1_all.map_partitions(\n",
    "    find_stats,\n",
    "    include_pixel=True,\n",
    "    # Keyword arguments after 'include_pixel=' are passed to your\n",
    "    # function\n",
    "    target_column=\"g_psfMag\",\n",
    ")\n",
    "npartitions = unrealized.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33d66f-0a7f-4238-80f3-51d439b076e4",
   "metadata": {},
   "source": [
    "While not too many for Data Preview 1, for full Rubin and other large surveys this can be a huge number of partitions! If we didn't bound this value, we could easily overwhelm our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6774bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Close the old client and make a new one that takes into account the new number of partitions.\n",
    "client.close()\n",
    "client = Client(n_workers=1, memory_limit=\"auto\") # 1 for RSP\n",
    "result = unrealized.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ed81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do a final reduction step to get the true min and max\n",
    "result[\"g_psfMag_min\"].min(), result[\"g_psfMag_max\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b0c69-cf65-4066-bbc7-662219e46085",
   "metadata": {},
   "source": [
    "Since we just searched the whole catalog, we can check our answer\n",
    "against the statistics that were compiled at import time for the\n",
    "catalog.  As you can see, they match what we got when using\n",
    "the `.map_partitions` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f919773",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1_all.hc_structure.aggregate_column_statistics(include_columns=\"g_psfMag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66913890-9300-4496-80bf-8a6abdb817c5",
   "metadata": {},
   "source": [
    "## Closing the Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf168b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0726ac",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "**Authors**: Derek Jones\n",
    "\n",
    "**Last updated on**: April 17, 2025\n",
    "\n",
    "If you use `lsdb` for published research, please cite following [instructions](https://docs.lsdb.io/en/stable/citation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b7c5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
