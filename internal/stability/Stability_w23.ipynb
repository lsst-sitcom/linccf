{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d575710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Astronomy-specific\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "# Distributed computing\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Nested pandas and HATS/LSDB\n",
    "import nested_pandas as npd\n",
    "from nested_pandas.utils import count_nested\n",
    "import lsdb\n",
    "from lsdb.core.search import ConeSearch\n",
    "import hats\n",
    "\n",
    "import lsst.daf.butler as dafButler\n",
    "from lsst.analysis.ap import apdb\n",
    "from lsst.ap.association import AssociationTask, AssociationConfig\n",
    "from lsst.dax.apdb import Apdb, ApdbCassandra, ApdbTables\n",
    "import lsst.geom as geom\n",
    "from lsst.afw import image as afwImage\n",
    "\n",
    "# Filesystem / cloud paths\n",
    "from upath import UPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"embargo\"\n",
    "collection = \"LSSTCam/runs/DRP/20250501_20250604/w_2025_23/DM-51284\"\n",
    "instrument = \"LSSTCam\"\n",
    "\n",
    "butler = dafButler.Butler(repo, collections=collection, instrument=instrument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b19a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_refs = butler.query_datasets(\"object\")\n",
    "lc_refs = butler.query_datasets(\"object_forced_source\")\n",
    "diaobj_refs = butler.query_datasets(\"dia_object\")\n",
    "dialc_refs = butler.query_datasets(\"dia_source\")\n",
    "\n",
    "print(\"Number of 'object' datasets:\", len(obj_refs))\n",
    "print(\"Number of 'object_forced_source' datasets:\", len(lc_refs))\n",
    "print(\"Number of 'dia_object' datasets:\", len(diaobj_refs))\n",
    "print(\"Number of 'dia_source' datasets:\", len(dialc_refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa766076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_ref_3163 = next(ref for ref in obj_refs if ref.dataId.get(\"tract\") == 3163)\n",
    "obj_ref_3163 = random.sample(obj_refs, 10)\n",
    "obj_ref_3163\n",
    "\n",
    "obj_tracts =  [ref.dataId[\"tract\"] for ref in obj_ref_3163]\n",
    "obj_tracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_refs = butler.query_datasets(\"isolated_star\")\n",
    "is_refs_in_tracts = [ref for ref in is_refs if ref.dataId.get(\"tract\") in obj_tracts]\n",
    "is_refs_in_tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c328a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_refs = butler.query_datasets(\"object_forced_source\")\n",
    "#lc_refs_3163 = [ref for ref in lc_refs if ref.dataId.get(\"tract\") == 3163]\n",
    "#lc_refs_3163\n",
    "# Filter to those with tract in obj_tracts\n",
    "lc_refs_in_tracts = [ref for ref in lc_refs if ref.dataId.get(\"tract\") in obj_tracts]\n",
    "\n",
    "# Sample 100 randomly (adjust number as needed)\n",
    "sample_size = min(500, len(lc_refs_in_tracts))  # avoid ValueError if fewer than 100\n",
    "lc_refs_3163 = random.sample(lc_refs_in_tracts, sample_size)\n",
    "\n",
    "print(f\"Sampled {len(lc_refs_3163)} object_forced_source refs from selected tracts.\")\n",
    "lc_refs_3163;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d15b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to hold object DataFrames\n",
    "object_list = []\n",
    "\n",
    "# Loop over tracts via obj_refs\n",
    "for forsor_ref in tqdm(obj_ref_3163):\n",
    "    try:\n",
    "        # Load full object table for this tract\n",
    "        table = butler.get(\"object\", dataId=forsor_ref.dataId)\n",
    "        # Convert to a pandas DataFrame\n",
    "        table=pd.DataFrame(table.to_pandas())\n",
    "        object_list.append(table)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: objectTable_tract not found for tract {forsor_ref.dataId['tract']}. Skipping.\")\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "object_list = pd.concat(object_list, ignore_index=False) if object_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff87e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to hold object DataFrames\n",
    "isolated_star_list = []\n",
    "\n",
    "# Loop over tracts via obj_refs\n",
    "for forsor_ref in tqdm(is_refs_in_tracts):\n",
    "    try:\n",
    "        # Load full object table for this tract\n",
    "        table = butler.get(\"isolated_star\", dataId=forsor_ref.dataId)\n",
    "        table=pd.DataFrame(table.to_pandas())\n",
    "        isolated_star_list.append(table)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: objectTable_tract not found for tract {forsor_ref.dataId['tract']}. Skipping.\")\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "isolated_star_list = pd.concat(isolated_star_list, ignore_index=False) if isolated_star_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169317b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_star_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a03e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source['band'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9af5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to hold object forced source  DataFrames\n",
    "object_forced_source_list = []\n",
    "\n",
    "# Loop over tracts via obj_refs\n",
    "for forsor_ref in tqdm(lc_refs_3163):\n",
    "    try:\n",
    "        # Load full object table for this tract\n",
    "        table = butler.get(\"object_forced_source\", dataId=forsor_ref.dataId)\n",
    "        # Convert to a pandas DataFrame\n",
    "        table=pd.DataFrame(table.to_pandas())\n",
    "        object_forced_source_list.append(table)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: objectTable_tract not found for tract {forsor_ref.dataId['tract']}. Skipping.\")\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "object_forced_source = pd.concat(object_forced_source_list, ignore_index=False) if object_forced_source_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"no_proxy\"] += \",.consdb\"\n",
    "from lsst.summit.utils import ConsDbClient\n",
    "token = 'gt-V4mYDSE4xUV72sN-2QIebw.bnPv71HEId3Oty-C46VtzA'\n",
    "client = ConsDbClient(f\"https://user:{token}@usdf-rsp.slac.stanford.edu/consdb\")\n",
    "visits = client.query(\n",
    "    \"SELECT * FROM cdb_lsstcam.visit1 WHERE day_obs >= 20250430 AND day_obs <= 20250605 and science_program = 'BLOCK-365'\"\n",
    ").to_pandas()\n",
    "visits['visit'] = visits['visit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b26be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source = object_forced_source.merge(visits[['visit', 'exp_midpt_mjd']], on='visit', how='left')\n",
    "\n",
    "# Drop rows where exp_midpt_mjd is missing\n",
    "object_forced_source = object_forced_source.dropna(subset=['exp_midpt_mjd'])\n",
    "\n",
    "# Now sort safely\n",
    "object_forced_source = object_forced_source.sort_values('exp_midpt_mjd')\n",
    "\n",
    "flag_columns = [col for col in object_forced_source.columns if 'Flag' in col]\n",
    "# Remove rows where any flag column is True\n",
    "object_forced_source = object_forced_source[~object_forced_source[flag_columns].any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source['ra'] = object_forced_source['coord_ra']\n",
    "object_forced_source['dec'] = object_forced_source['coord_dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e72f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(object_forced_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source_cat = lsdb.from_dataframe(object_forced_source, highest_order=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source_cat_isolated =lsdb.crossmatch(object_forced_source_cat, isolated_star_list, radius_arcsec=.1, suffixes=(\"\", \"_is\"))\n",
    "object_forced_source_cat_isolated = object_forced_source_cat_isolated.compute()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source = object_forced_source_cat_isolated[['objectId', 'coord_ra', 'exp_midpt_mjd', 'coord_dec', 'psfFlux', 'psfFluxErr', 'psfDiffFlux', 'psfDiffFluxErr', 'band']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# g-band\n",
    "\n",
    "object_forced_source_g = object_forced_source[object_forced_source['band'] == 'g']\n",
    "flag_columns = [col for col in object_forced_source.columns if 'Flag' in col]\n",
    "\n",
    "# Identify objectIds that appear more than once\n",
    "# basically select all objectIds that have more than 25 observations all together\n",
    "duplicate_ids = object_forced_source_g['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 25].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_g[object_forced_source_g['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "\n",
    "    # Skip if fewer than 20 points\n",
    "    if len(single_object) < 20:\n",
    "        print(f\"Skipping objectId {obj_id} with {len(single_object)} valid points.\")\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "    \n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_g = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f062675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r-band\n",
    "object_forced_source_r = object_forced_source[object_forced_source['band'] == 'r']\n",
    "\n",
    "# Identify objectIds that appear more than once\n",
    "duplicate_ids = object_forced_source_r['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 25].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_r[object_forced_source_r['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "    \n",
    "    # Skip if fewer than 20 points\n",
    "    if len(single_object) < 20:\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "    \n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_r = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u-band\n",
    "object_forced_source_u = object_forced_source[object_forced_source['band'] == 'u']\n",
    "flag_columns = [col for col in object_forced_source.columns if 'flag' in col.lower()]\n",
    "# Identify objectIds that appear more than once\n",
    "duplicate_ids = object_forced_source_u['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 5].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_u[object_forced_source_u['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "    \n",
    "    # Skip if fewer than 20 points\n",
    "    if len(single_object) < 10:\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "\n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_u = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c370cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source_i = object_forced_source[object_forced_source['band'] == 'i']\n",
    "print(len(object_forced_source_i))\n",
    "flag_columns = [col for col in object_forced_source.columns if 'flag' in col.lower()]\n",
    "# Identify objectIds that appear more than once\n",
    "duplicate_ids = object_forced_source_i['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 25].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_i[object_forced_source_i['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "\n",
    "    # Skip if fewer than 5 points\n",
    "    if len(single_object) < 20:\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "\n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_i = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44739362",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source_z = object_forced_source[object_forced_source['band'] == 'z']\n",
    "print(len(object_forced_source_z))\n",
    "flag_columns = [col for col in object_forced_source.columns if 'flag' in col.lower()]\n",
    "# Identify objectIds that appear more than once\n",
    "duplicate_ids = object_forced_source_z['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 25].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_i[object_forced_source_i['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "\n",
    "    # Skip if fewer than 20 points\n",
    "    if len(single_object) < 20:\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "\n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_z = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0062c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_forced_source_y = object_forced_source[object_forced_source['band'] == 'y']\n",
    "print(len(object_forced_source_y))\n",
    "flag_columns = [col for col in object_forced_source.columns if 'flag' in col.lower()]\n",
    "# Identify objectIds that appear more than once\n",
    "duplicate_ids = object_forced_source_y['objectId'].value_counts()\n",
    "duplicate_ids = duplicate_ids[duplicate_ids > 25].index\n",
    "\n",
    "object_list_band = object_list[object_list['objectId'].isin(duplicate_ids)]\n",
    "object_list_band_s = object_list_band[object_list_band['r_extendedness'] == False]\n",
    "duplicate_ids = object_list_band_s['objectId'].unique()\n",
    "\n",
    "# Select all rows with those objectIds\n",
    "duplicates_df = object_forced_source_i[object_forced_source_i['objectId'].isin(duplicate_ids)]\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(duplicates_df['objectId'].unique())} unique objectIds...\")\n",
    "for obj_id in tqdm(duplicates_df['objectId'].unique()[:10000]):\n",
    "    single_object = duplicates_df[duplicates_df['objectId'] == obj_id].copy()\n",
    "\n",
    "    # Skip if fewer than 5 points\n",
    "    if len(single_object) < 20:\n",
    "        continue\n",
    "    median_flux = single_object['psfFlux'].median()\n",
    "\n",
    "    # Select only  10 rows\n",
    "    single_object = single_object.sample(n=10, random_state=42)\n",
    "    mean_DiffErr = single_object['psfDiffFluxErr'].median()*np.sqrt(2)\n",
    "    mean_Err = single_object['psfFluxErr'].median()*np.sqrt(2)\n",
    "    diff_flux = np.diff(single_object['psfDiffFlux'])\n",
    "    delta_flux = np.diff(single_object['psfFlux'])\n",
    "    diff_time = np.diff(single_object['exp_midpt_mjd'])\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'objectId': obj_id,\n",
    "        'median_flux': median_flux,\n",
    "        'mean_psfDiffFluxError': mean_DiffErr,\n",
    "        'delta_psfDiffFlux': diff_flux,\n",
    "        'mean_psfFluxError': mean_Err,\n",
    "        'delta_psfFlux': delta_flux,\n",
    "        'delta_exp_midpt_mjd': diff_time\n",
    "    })\n",
    "\n",
    "    results.append(temp_df)\n",
    "\n",
    "# Combine results\n",
    "final_df_y = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171aada5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940aeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_df_u), len(final_df_g), len(final_df_r), len(final_df_i), len(final_df_z), len(final_df_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "\n",
    "def process_band_data_dia(final_df_band, label):\n",
    "    # Convert to AB magnitude\n",
    "    # take median flux of the object convert to magnitude, claim that as magnitude of the object \n",
    "    mag = u.nJy.to(u.ABmag, final_df_band['median_flux'])\n",
    "    final_df_band = final_df_band.copy()\n",
    "    # it is actually median mag, not mean\n",
    "    final_df_band['median_psfMag'] = mag\n",
    "\n",
    "    # Convert flux difference to magnitude difference\n",
    "    flux_delta = final_df_band['delta_psfDiffFlux'] + final_df_band['median_flux']\n",
    "    # magnitude of an individual source \n",
    "    mag_total_delta = u.nJy.to(u.ABmag, flux_delta)\n",
    "    final_df_band['mag_total_delta'] = mag_total_delta\n",
    "    final_df_band['mag_Diff_delta'] = final_df_band['mag_total_delta'] - final_df_band['median_psfMag']\n",
    "\n",
    "    # same night \n",
    "    short_diff = final_df_band[final_df_band['delta_exp_midpt_mjd'] < 0.5]\n",
    "    short_diff = short_diff[['median_psfMag', 'mag_Diff_delta', 'mean_psfDiffFluxError', 'median_flux']].dropna().copy()\n",
    "\n",
    "    # Bin by mean magnitude\n",
    "    bins = np.arange(16, 24.51, 0.5)\n",
    "    short_diff['mag_bin'] = pd.cut(short_diff['median_psfMag'], bins)\n",
    "\n",
    "    # Compute percentiles + mean error per bin\n",
    "    bin_centers, low_vals, med_vals, high_vals, err_vals = [], [], [], [], []\n",
    "\n",
    "    for bin_interval, group in short_diff.groupby('mag_bin'):\n",
    "        if group.empty or len(group) < 100:\n",
    "            continue\n",
    "        bin_center = (bin_interval.left + bin_interval.right) / 2\n",
    "        q16, q50, q84 = np.percentile(group['mag_Diff_delta'], [16, 50, 84])\n",
    "        mean_err_flux = group['mean_psfDiffFluxError'].mean()\n",
    "        mean_flux = group['median_flux'].mean()\n",
    "        mean_mag = u.nJy.to(u.ABmag, mean_flux)\n",
    "        mag_plus_err = u.nJy.to(u.ABmag, mean_flux + mean_err_flux)\n",
    "        mag_minus_err = u.nJy.to(u.ABmag, mean_flux - mean_err_flux)\n",
    "        mean_err_mag = float(abs(mag_plus_err - mag_minus_err)/2)\n",
    "\n",
    "        bin_centers.append(bin_center)\n",
    "        low_vals.append(q16)\n",
    "        med_vals.append(q50)\n",
    "        high_vals.append(q84)\n",
    "        err_vals.append(mean_err_mag)\n",
    "\n",
    "    return short_diff, bin_centers, low_vals, med_vals, high_vals, err_vals, label\n",
    "\n",
    "# Add all 6 bands\n",
    "bands = [\n",
    "    (final_df_u, 'u-band'),\n",
    "    (final_df_g, 'g-band'),\n",
    "    (final_df_r, 'r-band'),\n",
    "    (final_df_i, 'i-band'),\n",
    "    (final_df_z, 'z-band'),\n",
    "    (final_df_y, 'y-band')\n",
    "][:4]\n",
    "\n",
    "# 2x2 layout\n",
    "fig, axes = plt.subplots(2, 2, figsize=(21, 12), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (df, label) in zip(axes, bands):\n",
    "    short_diff, bin_centers, low_vals, med_vals, high_vals, err_vals, title = process_band_data_dia(df, label)\n",
    "\n",
    "    ax.scatter(short_diff['median_psfMag'], short_diff['mag_Diff_delta'], s=5, alpha=0.5, color='tab:blue', label='Data')\n",
    "    ax.plot(bin_centers, high_vals, color='red', linestyle='--', label='84th percentile')\n",
    "    ax.plot(bin_centers, med_vals, color='black', linestyle='-', label='50th percentile')\n",
    "    ax.plot(bin_centers, low_vals, color='red', linestyle='--', label='16th percentile')\n",
    "    \n",
    "\n",
    "    upper_err_line = [m + e for m, e in zip(med_vals, err_vals)]\n",
    "    lower_err_line = [m - e for m, e in zip(med_vals, err_vals)]\n",
    "\n",
    "    ax.plot(bin_centers, upper_err_line, color='orange', linestyle=':', lw=4, label='+median flux error')\n",
    "    ax.plot(bin_centers, lower_err_line, color='orange', linestyle=':', lw=4, label='–median flux error')\n",
    "\n",
    "\n",
    "    actual_spread = (np.array(high_vals) - np.array(med_vals) + np.array(med_vals) - np.array(low_vals))/2\n",
    "    delta_84_vs_err = np.sqrt(actual_spread**2 - np.array(err_vals)**2)\n",
    "    # delta_84_vs_err = [np.abs(q84 / upper) for q84, upper in zip(high_vals, upper_err_line)]\n",
    "\n",
    "    inset_ax = ax.inset_axes([0.05, 0.05, 0.4, 0.3])\n",
    "    inset_ax.plot(bin_centers, delta_84_vs_err, color='purple', marker='o', markersize=3, linewidth=1)\n",
    "    inset_ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    inset_ax.set_title(\"additional err needed\", fontsize=8)\n",
    "    inset_ax.tick_params(labelsize=7)\n",
    "    inset_ax.set_xlim(16, 24)\n",
    "    inset_ax.set_ylim(0.0, 0.05)\n",
    "    inset_ax.grid(True, linestyle='--', linewidth=0.3, alpha=0.6)\n",
    "\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel(\"Mean PSF Magnitude\", fontsize=12)\n",
    "    ax.set_xlim(16, 24)\n",
    "    ax.set_ylim(-0.2, 0.2)\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Shared y-labels for left column\n",
    "axes[0].set_ylabel(\"Δ Magnitude (psfDiffFlux)\", fontsize=12)\n",
    "axes[3].set_ylabel(\"Δ Magnitude (psfDiffFlux)\", fontsize=12)\n",
    "\n",
    "# Legend in the bottom-right plot\n",
    "axes[3].legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle(\"r_extendedness=0, forced observations, psfDiffFlux Δmag vs. Mean Mag (< 0.5 days)\", fontsize=15)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_spread = (np.array(high_vals) - np.array(med_vals) + np.array(med_vals) - np.array(low_vals))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d028e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(actual_spread**2 - np.array(err_vals)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(err_vals)**2 + 0.01**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_band_data(final_df_band, label):\n",
    "    # Convert to AB magnitude\n",
    "    # take median flux of the object convert to magnitude, claim that as magnitude of the object \n",
    "    mag = u.nJy.to(u.ABmag, final_df_band['median_flux'])\n",
    "    final_df_band = final_df_band.copy()\n",
    "    final_df_band['median_psfMag'] = mag\n",
    "\n",
    "    # Convert flux difference to magnitude difference\n",
    "    flux_delta = final_df_band['delta_psfFlux'] + final_df_band['median_flux']\n",
    "    # magnitude of an individual source \n",
    "    mag_total_delta = u.nJy.to(u.ABmag, flux_delta)\n",
    "    final_df_band['mag_total_delta'] = mag_total_delta\n",
    "    final_df_band['mag_delta'] = final_df_band['mag_total_delta'] - final_df_band['median_psfMag']\n",
    "\n",
    "    # same night \n",
    "    short_diff = final_df_band[final_df_band['delta_exp_midpt_mjd'] < 0.5]\n",
    "    short_diff = short_diff[['median_psfMag', 'mag_delta', 'mean_psfFluxError', 'median_flux']].dropna().copy()\n",
    "\n",
    "    # Bin by mean magnitude\n",
    "    bins = np.arange(16, 24.51, 0.5)\n",
    "    short_diff['mag_bin'] = pd.cut(short_diff['median_psfMag'], bins)\n",
    "\n",
    "    # Compute percentiles + mean error per bin\n",
    "    bin_centers, low_vals, med_vals, high_vals, err_vals = [], [], [], [], []\n",
    "\n",
    "    for bin_interval, group in short_diff.groupby('mag_bin'):\n",
    "        if group.empty or len(group) < 100:\n",
    "            continue\n",
    "        bin_center = (bin_interval.left + bin_interval.right) / 2\n",
    "        q16, q50, q84 = np.percentile(group['mag_delta'], [16, 50, 84])\n",
    "        mean_err_flux = group['mean_psfFluxError'].mean()\n",
    "        mean_flux = group['median_flux'].mean()\n",
    "        mean_mag = u.nJy.to(u.ABmag, mean_flux)\n",
    "        mag_plus_err = u.nJy.to(u.ABmag, mean_flux + mean_err_flux)\n",
    "        mag_minus_err = u.nJy.to(u.ABmag, mean_flux - mean_err_flux)\n",
    "        mean_err_mag = float(abs(mag_plus_err - mag_minus_err)/2)\n",
    "\n",
    "        bin_centers.append(bin_center)\n",
    "        low_vals.append(q16)\n",
    "        med_vals.append(q50)\n",
    "        high_vals.append(q84)\n",
    "        err_vals.append(mean_err_mag)\n",
    "\n",
    "    return short_diff, bin_centers, low_vals, med_vals, high_vals, err_vals, label\n",
    "\n",
    "# Add all 6 bands\n",
    "bands = [\n",
    "    (final_df_u, 'u-band'),\n",
    "    (final_df_g, 'g-band'),\n",
    "    (final_df_r, 'r-band'),\n",
    "    (final_df_i, 'i-band'),\n",
    "    (final_df_z, 'z-band'),\n",
    "    (final_df_y, 'y-band')\n",
    "][:4]\n",
    "\n",
    "# 2x3 layout\n",
    "fig, axes = plt.subplots(2, 2, figsize=(21, 12), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (df, label) in zip(axes, bands):\n",
    "    short_diff, bin_centers, low_vals, med_vals, high_vals, err_vals, title = process_band_data(df, label)\n",
    "\n",
    "    ax.scatter(short_diff['median_psfMag'], short_diff['mag_delta'], s=5, alpha=0.5, color='tab:blue', label='Data')\n",
    "    ax.plot(bin_centers, high_vals, color='red', linestyle='--', label='84th percentile')\n",
    "    ax.plot(bin_centers, med_vals, color='black', linestyle='-', label='50th percentile')\n",
    "    ax.plot(bin_centers, low_vals, color='red', linestyle='--', label='16th percentile')\n",
    "\n",
    "    upper_err_line = [m + e for m, e in zip(med_vals, err_vals)]\n",
    "    lower_err_line = [m - e for m, e in zip(med_vals, err_vals)]\n",
    "\n",
    "    ax.plot(bin_centers, upper_err_line, color='orange', linestyle=':', lw=4, label='+median flux error')\n",
    "    ax.plot(bin_centers, lower_err_line, color='orange', linestyle=':', lw=4, label='–median flux error')\n",
    "\n",
    "    actual_spread = (np.array(high_vals) - np.array(med_vals) + np.array(med_vals) - np.array(low_vals))/2\n",
    "    delta_84_vs_err = np.sqrt(actual_spread**2 - np.array(err_vals)**2)\n",
    "\n",
    "    inset_ax = ax.inset_axes([0.05, 0.05, 0.4, 0.3])\n",
    "    inset_ax.plot(bin_centers, delta_84_vs_err, color='purple', marker='o', markersize=3, linewidth=1)\n",
    "    inset_ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    inset_ax.set_title(\"obs_spread/err\", fontsize=8)\n",
    "    inset_ax.tick_params(labelsize=7)\n",
    "    inset_ax.set_xlim(16, 24)\n",
    "    inset_ax.set_ylim(0.0, 0.05)\n",
    "    inset_ax.grid(True, linestyle='--', linewidth=0.3, alpha=0.6)\n",
    "\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel(\"Mean PSF Magnitude\", fontsize=12)\n",
    "    ax.set_xlim(16, 24)\n",
    "    ax.set_ylim(-0.2, 0.2)\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Shared y-labels for left column\n",
    "axes[0].set_ylabel(\"Δ Magnitude (psfFlux)\", fontsize=12)\n",
    "axes[3].set_ylabel(\"Δ Magnitude (psfFlux)\", fontsize=12)\n",
    "\n",
    "# Legend in the bottom-right plot\n",
    "axes[3].legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle(\"r_extendedness=0, isolated stars, forced observations, psfFlux Δmag vs. Mean Mag (< 0.5 days)\", fontsize=15)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd30747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with 1 row, 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Band labels and colors\n",
    "bands = [\n",
    "    (final_df_u, 'u', 'blue'),\n",
    "    (final_df_g, 'g', 'green'),\n",
    "    (final_df_r, 'r', 'red'),\n",
    "    (final_df_i, 'i', 'orange'),\n",
    "    (final_df_z, 'z', 'brown'),\n",
    "    (final_df_y, 'y', 'purple')\n",
    "][:4]  # Limit to first 4 bands for clarity\n",
    "\n",
    "# Left panel: DIA\n",
    "for df, label, color in bands:\n",
    "    print(f\"Processing {label} band data...\")\n",
    "    _, bin_centers, low_vals, med_vals, high_vals, err_vals, _ = process_band_data_dia(df, label)\n",
    "\n",
    "    actual_spread = (np.array(high_vals) - np.array(med_vals) + np.array(med_vals) - np.array(low_vals)) / 2\n",
    "    delta_84_vs_err = np.sqrt(np.clip(actual_spread**2 - np.array(err_vals)**2, 0, None))\n",
    "    print(delta_84_vs_err)\n",
    "    axes[0].plot(bin_centers, delta_84_vs_err, marker='o', label=label, color=color, lw=2)\n",
    "\n",
    "axes[0].set_title(\"DIA: sqrt(obs_spread² - claimed_err²)\", fontsize=13)\n",
    "axes[0].set_xlabel(\"Median PSF Magnitude\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Mag\", fontsize=12)\n",
    "axes[0].set_xlim(16, 24)\n",
    "axes[0].set_ylim(0, 0.05)\n",
    "axes[0].grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "axes[0].legend(title=\"Band\", fontsize=10)\n",
    "\n",
    "# Right panel: reference\n",
    "for df, label, color in bands:\n",
    "    _, bin_centers, low_vals, med_vals, high_vals, err_vals, _ = process_band_data(df, label)\n",
    "\n",
    "    actual_spread = (np.array(high_vals) - np.array(med_vals) + np.array(med_vals) - np.array(low_vals)) / 2\n",
    "    delta_84_vs_err = np.sqrt(np.clip(actual_spread**2 - np.array(err_vals)**2, 0, None))  # clip to avoid sqrt of negative\n",
    "\n",
    "    axes[1].plot(bin_centers, delta_84_vs_err, marker='o', label=label, color=color, lw=2)\n",
    "\n",
    "axes[1].set_title(\"Reference: sqrt(obs_spread² - claimed_err²)\", fontsize=13)\n",
    "axes[1].set_xlabel(\"Median PSF Magnitude\", fontsize=12)\n",
    "axes[1].set_xlim(16, 24)\n",
    "axes[1].set_ylim(0, 0.05)\n",
    "axes[1].grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "axes[1].legend(title=\"Band\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Additional error needed to explain observations\", fontsize=15)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.array(err_vals) **2 + 0.02**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
