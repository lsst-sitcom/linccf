{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5060a38-77e8-44d6-8e0d-877edaa7268b",
   "metadata": {},
   "source": [
    "# Catalog inspection - By Field\n",
    "\n",
    "Perform more detailed verification on the datasets, using LSDB to inspect leaf parquet files, using spatial fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e490de7-923f-4870-83b1-302075eabfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lsdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.options.display.float_format = \"{:.4f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb02c256-f540-462d-90ce-50e1f02f193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = os.environ[\"VERSION\"]\n",
    "OUTPUT_DIR = Path(os.environ[\"OUTPUT_DIR\"])\n",
    "\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e6aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hats_dir = OUTPUT_DIR / \"hats\" / VERSION\n",
    "validation_dir = OUTPUT_DIR / \"validation\" / VERSION\n",
    "validation_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc19828-d3f3-4c9d-802d-b5a1c53d34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the six fields with their coordinates\n",
    "fields = {\n",
    "    \"ECDFS\": (53.13, -28.10),  # Extended Chandra Deep Field South\n",
    "    \"EDFS\": (59.10, -48.73),  # Euclid Deep Field South\n",
    "    \"Rubin_SV_38_7\": (37.86, 6.98),  # Low Ecliptic Latitude Field\n",
    "    \"Rubin_SV_95_-25\": (95.00, -25.00),  # Low Galactic Latitude Field\n",
    "    \"47_Tuc\": (6.02, -72.08),  # 47 Tuc Globular Cluster\n",
    "    \"Fornax_dSph\": (40.00, -34.45),  # Fornax Dwarf Spheroidal Galaxy\n",
    "}\n",
    "\n",
    "# Define the radius for selecting sources\n",
    "selection_radius_arcsec = 2.0 * 3600  # 2-degree radius\n",
    "\n",
    "# Define bands\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdfa0d8-bda1-4886-a834-460bb502caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute statistics for a given band\n",
    "def compute_band_stats(df, band, stat_columns):\n",
    "    \"\"\"Computes mean, min, max, and count of non-NaN values for a specific band.\"\"\"\n",
    "    mask = df[\"band\"] == band\n",
    "    stats = {}\n",
    "\n",
    "    if mask.sum() > 0:  # Ensure there are sources in this band\n",
    "        for col in stat_columns:\n",
    "            col_values = df.loc[mask, col]\n",
    "            stats[f\"mean_{col}_{band}\"] = np.nanmean(col_values)\n",
    "            stats[f\"min_{col}_{band}\"] = np.nanmin(col_values)\n",
    "            stats[f\"max_{col}_{band}\"] = np.nanmax(col_values)\n",
    "        if \"x\" in df.columns:\n",
    "            stats[f\"len_{band}\"] = mask.sum() - np.count_nonzero(\n",
    "                np.isnan(df.loc[mask, \"x\"])\n",
    "            )\n",
    "        else:\n",
    "            stats[f\"len_{band}\"] = mask.sum()\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Function to compute statistics for a given DataFrame\n",
    "def get_stats(df, stat_columns, out_columns):\n",
    "    \"\"\"Computes per-band statistics for a DataFrame, excluding sky sources.\"\"\"\n",
    "    if \"sky_source\" in df.columns:\n",
    "        df = df[df[\"sky_source\"] == False]  # Exclude rows where sky_source is True\n",
    "    stats = {col: np.nan for col in out_columns}\n",
    "\n",
    "    for band in bands:\n",
    "        stats.update(compute_band_stats(df, band, stat_columns))\n",
    "\n",
    "    return pd.DataFrame([stats])  # Convert to DataFrame\n",
    "\n",
    "\n",
    "# Function to compute weighted statistics across fields\n",
    "def compute_weighted_stats(result, column_names, bands):\n",
    "    \"\"\"Computes weighted mean, min, and max statistics for each band.\"\"\"\n",
    "    weighted_stats = {}\n",
    "\n",
    "    for column, band in itertools.product(column_names, bands):\n",
    "        len_band = np.nansum(result[f\"len_{band}\"])\n",
    "        if len_band:\n",
    "            mean_col_name = f\"mean_{column}_{band}\"\n",
    "            min_col_name = f\"min_{column}_{band}\"\n",
    "            max_col_name = f\"max_{column}_{band}\"\n",
    "\n",
    "            # Compute weighted mean\n",
    "            weighted_stats[mean_col_name] = (\n",
    "                np.nansum(result[mean_col_name] * result[f\"len_{band}\"]) / len_band\n",
    "            )\n",
    "\n",
    "            # Compute min and max directly\n",
    "            weighted_stats[min_col_name] = np.nanmin(result[min_col_name])\n",
    "            weighted_stats[max_col_name] = np.nanmax(result[max_col_name])\n",
    "\n",
    "    return weighted_stats\n",
    "\n",
    "\n",
    "def run_weighted_statistics(cat):\n",
    "\n",
    "    ## What are the columns of interest for the results? Everything numeric!\n",
    "    print(\"starting column count\", len(cat._ddf.meta.columns))\n",
    "    column_names = list(cat._ddf.meta.select_dtypes(include=np.number))\n",
    "\n",
    "    # Exclude HATS-added columns\n",
    "    column_names = [\n",
    "        c for c in column_names if c not in [\"_healpix_29\", \"Norder\", \"Dir\", \"Npix\"]\n",
    "    ]\n",
    "    column_names = [c for c in column_names if not c.endswith(\"Id\")]\n",
    "    column_names = [c for c in column_names if \"Mag\" not in c]\n",
    "    print(\"effective column count\", len(column_names))\n",
    "\n",
    "    # Create meta dictionary for Dask\n",
    "    meta = {}\n",
    "    for column, band in itertools.product(column_names, bands):\n",
    "        meta[f\"mean_{column}_{band}\"] = \"f8\"\n",
    "        meta[f\"min_{column}_{band}\"] = \"f8\"\n",
    "        meta[f\"max_{column}_{band}\"] = \"f8\"\n",
    "    for band in bands:\n",
    "        meta[f\"len_{band}\"] = \"i8\"\n",
    "\n",
    "    # Dictionary to store results\n",
    "    all_results = {}\n",
    "\n",
    "    # Loop through each field and perform cone search + computation\n",
    "    for field_name, (ra, dec) in tqdm(fields.items()):\n",
    "        # Perform cone search for the given field\n",
    "        field_cat = cat.cone_search(\n",
    "            ra=ra, dec=dec, radius_arcsec=selection_radius_arcsec\n",
    "        )\n",
    "\n",
    "        # Compute statistics using Dask\n",
    "        result = field_cat.map_partitions(\n",
    "            get_stats,\n",
    "            meta=meta,\n",
    "            stat_columns=column_names,\n",
    "            out_columns=meta.keys(),\n",
    "        ).compute()\n",
    "\n",
    "        # Compute weighted statistics for the field\n",
    "        all_results[field_name] = compute_weighted_stats(result, column_names, bands)\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    return pd.DataFrame.from_dict(all_results, orient=\"index\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e5918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omitting.  A numeric 'band' is not in 'source2'\n",
    "# cat = lsdb.read_hats(hats_dir / \"source2\")\n",
    "# final_statistics = run_weighted_statistics(cat)\n",
    "# final_statistics.to_parquet(validation_dir / \"source_byfield.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a07225-3198-4c03-bf4b-5e719fef29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = lsdb.read_hats(hats_dir / \"object_forced_source\")\n",
    "final_statistics = run_weighted_statistics(cat)\n",
    "final_statistics.to_parquet(validation_dir / \"object_forced_source_byfield.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f1aa3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = lsdb.read_hats(hats_dir / \"dia_source\")\n",
    "final_statistics = run_weighted_statistics(cat)\n",
    "final_statistics.to_parquet(validation_dir / \"dia_source_byfield.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75e8b7a-e7f6-4f15-ad6e-7ca92ff1b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = lsdb.read_hats(hats_dir / \"dia_object_forced_source\")\n",
    "final_statistics = run_weighted_statistics(cat)\n",
    "final_statistics.to_parquet(validation_dir / \"dia_object_forced_source_byfield.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
