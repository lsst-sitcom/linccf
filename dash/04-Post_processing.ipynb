{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e2eca1-88a2-493c-8901-98828d0faae1",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "\n",
    "We will modify each parquet file in place. This seems like a good idea today, but could be crap tomorrow.\n",
    "\n",
    "If we use LSDB, we will need to use additional disk storage, both for fresh and post-processed data.\n",
    "\n",
    "Elements of post-processing to be accomplished in this notebook:\n",
    "\n",
    "* brightness in magnitude (e.g. convert ALL flux to magnitude)\n",
    "* join to visit table, where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440e22ef-a3a2-4bfb-9f84-d6f1cfc17fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:18:58.192856Z",
     "iopub.status.busy": "2025-01-30T17:18:58.192617Z",
     "iopub.status.idle": "2025-01-30T17:18:58.195383Z",
     "shell.execute_reply": "2025-01-30T17:18:58.195045Z",
     "shell.execute_reply.started": "2025-01-30T17:18:58.192842Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import astropy.units as u\n",
    "import hats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import tempfile\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from dask.distributed import as_completed, Client\n",
    "from hats.catalog import PartitionInfo\n",
    "from hats.io import paths\n",
    "from hats.io.parquet_metadata import write_parquet_metadata\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f10d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRP_VERSION = os.environ[\"DRP_VERSION\"]\n",
    "print(f\"DRP_VERSION: {DRP_VERSION}\")\n",
    "base_output_dir = Path(f\"/sdf/data/rubin/shared/lsdb_commissioning\")\n",
    "raw_dir = base_output_dir / \"raw\" / DRP_VERSION\n",
    "hats_dir = base_output_dir / \"hats\" / DRP_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623de244",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_table = pd.read_parquet(raw_dir / \"visit_table.parquet\")\n",
    "visit_map = visit_table.set_index(\"visitId\")[\"expMidptMJD\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f838266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_mag_and_magerr(table, flux_col_prefixes):\n",
    "    \"\"\"Calculate magnitudes and their errors for flux columns.\"\"\"\n",
    "    mag_cols = {}\n",
    "    for prefix in flux_col_prefixes:\n",
    "        # Magnitude\n",
    "        flux = table[f\"{prefix}Flux\"]\n",
    "        mag = u.nJy.to(u.ABmag, flux)\n",
    "        mag_cols[f\"{prefix}Mag\"] = mag\n",
    "        # Magnitude error, if flux error exists\n",
    "        fluxErr_col = f\"{prefix}FluxErr\"\n",
    "        if fluxErr_col in table.columns:\n",
    "            fluxErr = table[fluxErr_col]\n",
    "            upper_mag = u.nJy.to(u.ABmag, flux + fluxErr)\n",
    "            lower_mag = u.nJy.to(u.ABmag, flux - fluxErr)\n",
    "            magErr = -(upper_mag - lower_mag) / 2\n",
    "            mag_cols[f\"{prefix}MagErr\"] = magErr\n",
    "    mag_table = pd.DataFrame(mag_cols, dtype=np.float32, index=table.index)\n",
    "    return pd.concat([table, mag_table], axis=1)\n",
    "\n",
    "\n",
    "def add_mjd_from_visit(table):\n",
    "    \"\"\"Add mjd (if it does not exist) from the visit mapping\"\"\"\n",
    "    if \"visit\" not in table.columns:\n",
    "        raise ValueError(\"`visit` column is missing\")\n",
    "    if \"midpointMjdTai\" in table.columns:\n",
    "        raise ValueError(\"`mjd` is already present in table\")\n",
    "    mjds = list(map(lambda x: visit_map.get(x, np.nan), table[\"visit\"]))\n",
    "    table[\"midpointMjdTai\"] = pd.Series(mjds, index=table.index)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c049d",
   "metadata": {},
   "source": [
    "Initialize a Dask Client to parallelize the post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9415b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "client = Client(n_workers=16, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eaa2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_catalog(catalog_name, flux_col_prefixes=[], add_mjds=False):\n",
    "    catalog_dir = hats_dir / catalog_name\n",
    "    catalog = hats.read_hats(catalog_dir)\n",
    "    futures = []\n",
    "    for target_pixel in catalog.get_healpix_pixels():\n",
    "        futures.append(\n",
    "            client.submit(\n",
    "                process_partition,\n",
    "                catalog_dir=catalog_dir,\n",
    "                target_pixel=target_pixel,\n",
    "                flux_col_prefixes=flux_col_prefixes,\n",
    "                add_mjds=add_mjds,\n",
    "            )\n",
    "        )\n",
    "    wait_for_futures(futures, catalog_name)\n",
    "    rewrite_catalog_metadata(catalog)\n",
    "\n",
    "\n",
    "def process_partition(catalog_dir, target_pixel, flux_col_prefixes, add_mjds):\n",
    "    \"\"\"Apply post-processing steps to each individual partition\"\"\"\n",
    "    file_path = hats.io.pixel_catalog_file(catalog_dir, target_pixel)\n",
    "    table = pd.read_parquet(file_path)\n",
    "    # Add magnitudes and mjds\n",
    "    if len(flux_col_prefixes) > 0:\n",
    "        table = append_mag_and_magerr(table, flux_col_prefixes)\n",
    "    if add_mjds:\n",
    "        table = add_mjd_from_visit(table)\n",
    "    # Overwrite partition on disk\n",
    "    final_table = pa.Table.from_pandas(\n",
    "        table, preserve_index=False\n",
    "    ).replace_schema_metadata()\n",
    "    pq.write_table(final_table, file_path.path)\n",
    "\n",
    "\n",
    "def wait_for_futures(futures, catalog_name):\n",
    "    for future in tqdm(as_completed(futures), desc=catalog_name, total=len(futures)):\n",
    "        if future.status == \"error\":\n",
    "            raise future.exception()\n",
    "\n",
    "\n",
    "def rewrite_catalog_metadata(catalog):\n",
    "    \"\"\"Update catalog metadata after processing the leaf parquet files\"\"\"\n",
    "    destination_path = hats_dir / catalog.catalog_name\n",
    "\n",
    "    parquet_rows = write_parquet_metadata(destination_path)\n",
    "\n",
    "    # Read partition info from _metadata and write to partition_info.csv\n",
    "    partition_info = PartitionInfo.read_from_dir(destination_path)\n",
    "    partition_info_file = paths.get_partition_info_pointer(destination_path)\n",
    "    partition_info.write_to_file(partition_info_file)\n",
    "\n",
    "    now = datetime.now(tz=timezone.utc)\n",
    "\n",
    "    catalog.catalog_info.copy_and_update(\n",
    "        total_rows=parquet_rows, hats_creation_date=now.strftime(\"%Y-%m-%dT%H:%M%Z\")\n",
    "    ).to_properties_file(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd17a8c-01cd-4caf-9a5c-b61cd44de04e",
   "metadata": {},
   "source": [
    "## dia_object\n",
    "\n",
    "This one is the easiest because it doesn't require ANY post-processing!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c9c76-f6c2-441f-9fcb-278766fd3c4c",
   "metadata": {},
   "source": [
    "## dia_source\n",
    "\n",
    "We need to add the psf/science magnitudes and their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6992be51-842b-4681-9c59-1c6ba0d4da0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:18:50.535150Z",
     "iopub.status.busy": "2025-01-30T17:18:50.534718Z",
     "iopub.status.idle": "2025-01-30T17:18:50.573614Z",
     "shell.execute_reply": "2025-01-30T17:18:50.573248Z",
     "shell.execute_reply.started": "2025-01-30T17:18:50.535134Z"
    }
   },
   "outputs": [],
   "source": [
    "postprocess_catalog(\"dia_source\", flux_col_prefixes=[\"psf\", \"science\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56622dd",
   "metadata": {},
   "source": [
    "## dia_object_forced_source\n",
    "\n",
    "We need to add the psf magnitudes and their errors.\n",
    "\n",
    "We add the `midpointMjdTai` from the visits table lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a6c134-d46d-4386-a541-91d4f63d130e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:19:00.943321Z",
     "iopub.status.busy": "2025-01-30T17:19:00.942915Z",
     "iopub.status.idle": "2025-01-30T17:19:01.590679Z",
     "shell.execute_reply": "2025-01-30T17:19:01.590230Z",
     "shell.execute_reply.started": "2025-01-30T17:19:00.943305Z"
    }
   },
   "outputs": [],
   "source": [
    "postprocess_catalog(\n",
    "    \"dia_object_forced_source\", flux_col_prefixes=[\"psf\"], add_mjds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57203510",
   "metadata": {},
   "source": [
    "## object\n",
    "\n",
    "We need to add the psf/kron magnitudes, for each band, and their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720262be-d054-402b-adf1-5859f4be273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_col_prefixes = []\n",
    "for band in list(\"ugrizy\"):\n",
    "    for flux_name in [\"psf\", \"kron\"]:\n",
    "        band_col = f\"{band}_{flux_name}\"\n",
    "        flux_col_prefixes.append(band_col)\n",
    "print(flux_col_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1236bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_catalog(\"object\", flux_col_prefixes=flux_col_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a5229",
   "metadata": {},
   "source": [
    "## source\n",
    "\n",
    "We need to add the psf magnitudes and their errors.\n",
    "\n",
    "We add the `midpointMjdTai` from the visits table lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "443d798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_catalog(\"source\", flux_col_prefixes=[\"psf\"], add_mjds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969020c",
   "metadata": {},
   "source": [
    "## object_forced_source\n",
    "\n",
    "We need to add the psf magnitudes and their errors.\n",
    "\n",
    "We add the `midpointMjdTai` from the visits table lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af395e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_catalog(\"object_forced_source\", flux_col_prefixes=[\"psf\"], add_mjds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73481c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "tmp_path.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
