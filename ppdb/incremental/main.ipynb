{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7544b18c",
   "metadata": {},
   "source": [
    "## Incremental imports\n",
    "\n",
    "```bash\n",
    "__ dia_object_collection/\n",
    "|__ dia_object_lc/\n",
    "   |__ dataset/\n",
    "       |__ Norder=1/\n",
    "       |   |__ Dir=0/\n",
    "       |       |__ Npix=0/\n",
    "       |           |__ 2025-09-26.parquet\n",
    "       |           |__ 2025-09-27.parquet\n",
    "       |           |__ past.parquet (weekly_aggregated)\n",
    "       |__ Norder=2/\n",
    "       |   |__ Dir=0/\n",
    "       |       |__ Npix=0/\n",
    "       |           |__ 2025-09-26.parquet\n",
    "       |           |__ 2025-09-27.parquet\n",
    "       |           |__ past.parquet (weekly_aggregated)\n",
    "       |__ .../\n",
    "       |__ _common_metadata (constant)\n",
    "   |__ partition_info.csv (constant)\n",
    "   |__ hats.properties (daily_updated)\n",
    "```\n",
    "\n",
    "#### Daily\n",
    "\n",
    "- Keep the existing partitioning schema.\n",
    "\n",
    "- Add a new parquet file to each pixel.\n",
    "\n",
    "  - Account for the object rows of latest validity start only.\n",
    "  \n",
    "  - Find out where each new object lies.\n",
    "\n",
    "  - Generate the margin caches for sources and forced sources.\n",
    "  \n",
    "  - Nest the sources and forced sources in their respective objects.\n",
    "\n",
    "  - Copy the new daily parquet files into their respective pixel directories.\n",
    "\n",
    "- Delete the outdated _metadata and skymaps, if they exist.\n",
    "\n",
    "- Update hats.properties (at least `n_rows`).\n",
    "\n",
    "\n",
    "#### [TBD] Every so often (e.g. weekly):\n",
    "\n",
    "- Aggregate the daily parquets of each pixel into the `past.parquet` file.\n",
    "\n",
    "  - Merge duplicate object rows.\n",
    "\n",
    "  - Keep the latest values for each conflicting object columns.\n",
    "\n",
    "  - Merge the light curve nested columns.\n",
    "\n",
    "- Repartition each pixel according to a pre-defined threshold argument.\n",
    "\n",
    "- Regenerate _metadata and skymap files.\n",
    "\n",
    "- Regenerate collection with margin cache and index catalog (from scratch).\n",
    "\n",
    "\n",
    "Similar to DB transactions, these updates are disruptive actions. Users should be OK with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bae7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook requires calling `Catalog.join_nested` with \"how='left'\"\n",
    "# %pip install git+https://github.com/astronomy-commons/lsdb.git@sandro/join-nested-left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a64414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import tempfile\n",
    "\n",
    "from dask.distributed import Client\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "from hats_import.catalog.resume_plan import ResumePlan\n",
    "from incremental.pipeline import map_pixels, binning, split_pixels, reduce_pixels, finalize\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99730c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PPDB data\n",
    "PPDB_DIR = Path(\"/sdf/scratch/rubin/ppdb/data/lsstcam\")\n",
    "\n",
    "# Path to a pre-existing catalog\n",
    "dia_object_lc_path = Path(\"incremental/dia_object_lc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Dask Client\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "client = Client(n_workers=16, threads_per_worker=1, local_directory=tmp_dir.name)\n",
    "tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e91d12",
   "metadata": {},
   "source": [
    "Load existing nested catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The catalog has leaf HEALPix directories\n",
    "dia_object_lc = lsdb.open_catalog(dia_object_lc_path)\n",
    "existing_pixels = set(dia_object_lc.get_healpix_pixels())\n",
    "existing_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6553dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the highest order of the existing \n",
    "# nested catalog to map the new data\n",
    "mapping_order = int(max(existing_pixels).order)\n",
    "print(f\"mapping_order = {mapping_order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f918a33",
   "metadata": {},
   "source": [
    "Get the new PPDB file increments for objects, sources and forced sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ae367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(dataset_type, start=10, end=20):\n",
    "    \"\"\"Return the latest parquet file for a given dataset type.\"\"\"\n",
    "    dataset_name = \"\".join(word.capitalize() for word in dataset_type.split(\"_\"))\n",
    "    files = sorted(PPDB_DIR.rglob(f\"{dataset_name}.parquet\"))\n",
    "    return files[start:end]\n",
    "\n",
    "new_object_files = get_paths(\"dia_object\")\n",
    "new_source_files = get_paths(\"dia_source\")\n",
    "new_forced_source_files = get_paths(\"dia_forced_source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cd9ef",
   "metadata": {},
   "source": [
    "When importing new data, we want to keep the existing partitioning structure intact. Weâ€™ll only add new pixels if the alerts include data that lies outside the current pixel coverage. This is a change from how the `hats-import` map-reduce pipeline currently works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6db0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(dataset_name, catalog_type, new_input_files):\n",
    "    args = ImportArguments(\n",
    "        output_path=tmp_dir.name,\n",
    "        output_artifact_name=dataset_name,\n",
    "        input_file_list=new_input_files,\n",
    "        file_reader=\"parquet\",\n",
    "        ra_column=\"ra\",\n",
    "        dec_column=\"dec\",\n",
    "        catalog_type=catalog_type,\n",
    "        highest_healpix_order=mapping_order,\n",
    "        simple_progress_bar=True,\n",
    "        resume=False,\n",
    "    )\n",
    "    resume_plan = ResumePlan(import_args=args)\n",
    "    histogram, total_rows, pickled_reader_file = map_pixels(args, resume_plan, client)\n",
    "    alignment_file = binning(args, resume_plan, histogram, existing_pixels, total_rows)\n",
    "    split_pixels(args, resume_plan, alignment_file, pickled_reader_file, client)\n",
    "    reduce_pixels(args, resume_plan, client)\n",
    "    finalize(args, resume_plan, histogram, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset(\"dia_object\", \"object\", new_object_files)\n",
    "import_dataset(\"dia_source\", \"source\", new_source_files)\n",
    "import_dataset(\"dia_forced_source\", \"source\", new_forced_source_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08f31d",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "Same post-processing steps that were applied to the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37870ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import hats\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from dask.distributed import as_completed\n",
    "from datetime import datetime, timezone\n",
    "from hats.catalog import PartitionInfo\n",
    "from hats.io import paths\n",
    "from hats.io.parquet_metadata import write_parquet_metadata\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c46545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_latest_validity(table):\n",
    "    \"\"\"Select rows with the latest validityStart for each object.\"\"\"\n",
    "    return table.sort_values(\"validityStart\").drop_duplicates(\n",
    "        \"diaObjectId\", keep=\"last\"\n",
    "    )\n",
    "\n",
    "\n",
    "def append_mag_and_magerr(table, flux_cols):\n",
    "    \"\"\"Calculate magnitudes and their errors for flux columns.\"\"\"\n",
    "    mag_cols = {}\n",
    "\n",
    "    for flux_col in flux_cols:\n",
    "        flux_col_err = f\"{flux_col}Err\"\n",
    "        mag_col = flux_col.replace(\"Flux\", \"Mag\")\n",
    "        mag_col_err = f\"{mag_col}Err\"\n",
    "\n",
    "        flux = table[flux_col]\n",
    "        mag = u.nJy.to(u.ABmag, flux)\n",
    "        mag_cols[mag_col] = mag\n",
    "\n",
    "        flux_err = table[flux_col_err]\n",
    "        upper_mag = u.nJy.to(u.ABmag, flux + flux_err)\n",
    "        lower_mag = u.nJy.to(u.ABmag, flux - flux_err)\n",
    "        magErr = -(upper_mag - lower_mag) / 2\n",
    "        mag_cols[mag_col_err] = magErr\n",
    "\n",
    "    mag_table = pd.DataFrame(\n",
    "        mag_cols, dtype=pd.ArrowDtype(pa.float32()), index=table.index\n",
    "    )\n",
    "    return pd.concat([table, mag_table], axis=1)\n",
    "\n",
    "\n",
    "def cast_columns_float32(table):\n",
    "    \"\"\"Cast non-(positional/time) columns to single-precision\"\"\"\n",
    "    position_time_cols = [\n",
    "        \"ra\",\n",
    "        \"dec\",\n",
    "        \"raErr\",\n",
    "        \"decErr\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"xErr\",\n",
    "        \"yErr\",\n",
    "        \"midpointMjdTai\",\n",
    "        \"radecMjdTai\",\n",
    "    ]\n",
    "    columns_to_cast = [\n",
    "        field\n",
    "        for (field, type) in table.dtypes.items()\n",
    "        if field not in position_time_cols and type == pd.ArrowDtype(pa.float64())\n",
    "    ]\n",
    "    dtype_map = {col: pd.ArrowDtype(pa.float32()) for col in columns_to_cast}\n",
    "    return table.astype(dtype_map)\n",
    "\n",
    "\n",
    "def postprocess_catalog(catalog_name, flux_col_prefixes):\n",
    "    catalog_dir = f\"{tmp_dir.name}/{catalog_name}\"\n",
    "    catalog = hats.read_hats(catalog_dir)\n",
    "    futures = []\n",
    "    for target_pixel in catalog.get_healpix_pixels():\n",
    "        futures.append(\n",
    "            client.submit(\n",
    "                process_partition,\n",
    "                catalog_dir=catalog_dir,\n",
    "                target_pixel=target_pixel,\n",
    "                flux_col_prefixes=flux_col_prefixes,\n",
    "            )\n",
    "        )\n",
    "    for future in tqdm(as_completed(futures), desc=catalog_name, total=len(futures)):\n",
    "        if future.status == \"error\":\n",
    "            raise future.exception()\n",
    "    rewrite_catalog_metadata(catalog)\n",
    "\n",
    "\n",
    "def process_partition(catalog_dir, target_pixel, flux_col_prefixes):\n",
    "    \"\"\"Apply post-processing steps to each individual partition\"\"\"\n",
    "    file_path = hats.io.pixel_catalog_file(catalog_dir, target_pixel)\n",
    "    table = pd.read_parquet(file_path, dtype_backend=\"pyarrow\")\n",
    "    if \"validityStart\" in table.columns:\n",
    "        table = select_by_latest_validity(table)\n",
    "    if len(flux_col_prefixes) > 0:\n",
    "        table = append_mag_and_magerr(table, flux_col_prefixes)\n",
    "    table = cast_columns_float32(table)\n",
    "    final_table = pa.Table.from_pandas(\n",
    "        table, preserve_index=False\n",
    "    ).replace_schema_metadata()\n",
    "    pq.write_table(final_table, file_path.path)\n",
    "\n",
    "\n",
    "def rewrite_catalog_metadata(catalog):\n",
    "    \"\"\"Update catalog metadata after processing the leaf parquet files\"\"\"\n",
    "    destination_path = f\"{tmp_dir.name}/{catalog.catalog_name}\"\n",
    "    parquet_rows = write_parquet_metadata(destination_path)\n",
    "    # Read partition info from _metadata and write to partition_info.csv\n",
    "    partition_info = PartitionInfo.read_from_dir(destination_path)\n",
    "    partition_info_file = paths.get_partition_info_pointer(destination_path)\n",
    "    partition_info.write_to_file(partition_info_file)\n",
    "    now = datetime.now(tz=timezone.utc)\n",
    "    catalog.catalog_info.copy_and_update(\n",
    "        total_rows=parquet_rows, hats_creation_date=now.strftime(\"%Y-%m-%dT%H:%M%Z\")\n",
    "    ).to_properties_file(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_col_prefixes = [f\"{band}_scienceFluxMean\" for band in list(\"ugrizy\")]\n",
    "postprocess_catalog(\"dia_object\", flux_col_prefixes=flux_col_prefixes)\n",
    "postprocess_catalog(\"dia_source\", flux_col_prefixes=[\"scienceFlux\"])\n",
    "postprocess_catalog(\"dia_forced_source\", flux_col_prefixes=[\"scienceFlux\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cbe41",
   "metadata": {},
   "source": [
    "### Create nested increment\n",
    "\n",
    "Nest sources in objects and sort them by MJD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hats_import import pipeline_with_client\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "\n",
    "margin_radius_arcsec = 5\n",
    "\n",
    "def sort_nested_sources(df, source_cols):\n",
    "    mjd_col = \"midpointMjdTai\"\n",
    "    for source_col in source_cols:\n",
    "        flat_sources = df[source_col].nest.to_flat()\n",
    "        df = df.drop(columns=[source_col])\n",
    "        df = df.add_nested(\n",
    "            flat_sources.sort_values([flat_sources.index.name, mjd_col]), source_col\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = MarginCacheArguments(\n",
    "    input_catalog_path=f\"{tmp_dir.name}/dia_source\",\n",
    "    output_path=tmp_dir.name,\n",
    "    margin_threshold=margin_radius_arcsec,\n",
    "    output_artifact_name=f\"dia_source_{margin_radius_arcsec}arcs\",\n",
    "    simple_progress_bar=True,\n",
    "    resume=False,\n",
    ")\n",
    "pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = MarginCacheArguments(\n",
    "    input_catalog_path=f\"{tmp_dir.name}/dia_forced_source\",\n",
    "    output_path=tmp_dir.name,\n",
    "    margin_threshold=margin_radius_arcsec,\n",
    "    output_artifact_name=f\"dia_forced_source_{margin_radius_arcsec}arcs\",\n",
    "    simple_progress_bar=True,\n",
    "    resume=False,\n",
    ")\n",
    "pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_object = lsdb.read_hats(f\"{tmp_dir.name}/dia_object\")\n",
    "\n",
    "dia_source = lsdb.read_hats(\n",
    "    f\"{tmp_dir.name}/dia_source\",\n",
    "    margin_cache=f\"{tmp_dir.name}/dia_source_{margin_radius_arcsec}arcs\",\n",
    ")\n",
    "\n",
    "dia_forced_source = lsdb.read_hats(\n",
    "    f\"{tmp_dir.name}/dia_forced_source\",\n",
    "    margin_cache=f\"{tmp_dir.name}/dia_forced_source_{margin_radius_arcsec}arcs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe76e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_object_nested = (\n",
    "    dia_object.join_nested(\n",
    "        dia_source,\n",
    "        left_on=\"diaObjectId\",\n",
    "        right_on=\"diaObjectId\",\n",
    "        nested_column_name=\"diaSource\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join_nested(\n",
    "        dia_forced_source,\n",
    "        left_on=\"diaObjectId\",\n",
    "        right_on=\"diaObjectId\",\n",
    "        nested_column_name=\"diaForcedSource\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .map_partitions(\n",
    "        lambda x: sort_nested_sources(x, source_cols=[\"diaSource\", \"diaForcedSource\"])\n",
    "    )\n",
    ")\n",
    "dia_object_nested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61f536",
   "metadata": {},
   "source": [
    "### Update existing catalog\n",
    "\n",
    "Write the partitions with the new data to the existing catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_partitions(catalog, output_catalog_dir, **kwargs):\n",
    "    \"\"\"Saves catalog partitions as parquet to disk\"\"\"\n",
    "    results, pixels = [], []\n",
    "    partitions = catalog._ddf.to_delayed()\n",
    "\n",
    "    # The new parquet files will be named after the current date.\n",
    "    npix_suffix = f\"/{datetime.now().strftime(\"%Y-%m-%d\")}.parquet\"\n",
    "\n",
    "    for pixel, partition_index in catalog._ddf_pixel_map.items():\n",
    "        results.append(\n",
    "            perform_write(\n",
    "                partitions[partition_index],\n",
    "                pixel,\n",
    "                output_catalog_dir,\n",
    "                npix_suffix,\n",
    "                **kwargs,\n",
    "            )\n",
    "        )\n",
    "        pixels.append(pixel)\n",
    "\n",
    "    counts = dask.compute(*results)\n",
    "    non_empty_indices = np.nonzero(counts)\n",
    "    non_empty_pixels = np.array(pixels)[non_empty_indices]\n",
    "    non_empty_counts = np.array(counts)[non_empty_indices]\n",
    "\n",
    "    # Check that the catalog is not empty\n",
    "    if len(non_empty_pixels) == 0:\n",
    "        raise RuntimeError(\"The output catalog is empty\")\n",
    "    return list(non_empty_pixels), list(non_empty_counts)\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def perform_write(df, hp_pixel, output_catalog_dir, npix_suffix, **kwargs):\n",
    "    if len(df) == 0:\n",
    "        return 0    \n",
    "    # The parquet leaf files live in a pixel directory. Create it if it does not exist.\n",
    "    pixel_dir = hats.io.pixel_directory(output_catalog_dir, hp_pixel.order, hp_pixel.pixel) / f\"Npix={hp_pixel.pixel}\"\n",
    "    hats.io.file_io.make_directory(pixel_dir, exist_ok=True)\n",
    "    # Write file to destination.\n",
    "    pixel_path = paths.pixel_catalog_file(output_catalog_dir, hp_pixel, npix_suffix=npix_suffix)\n",
    "    df.to_parquet(pixel_path.path, filesystem=pixel_path.fs, **kwargs)\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04150774",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels, counts = write_partitions(dia_object_nested, dia_object_lc_path)\n",
    "pixels, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf7af6",
   "metadata": {},
   "source": [
    "And update the catalog properties and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete point maps and sky maps\n",
    "paths.get_skymap_file_pointer(dia_object_lc_path).unlink(missing_ok=True)\n",
    "paths.get_point_map_file_pointer(dia_object_lc_path).unlink(missing_ok=True)\n",
    "for order in dia_object_lc.hc_structure.catalog_info.skymap_alt_orders:\n",
    "    paths.get_skymap_file_pointer(dia_object_lc_path, order=order).unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete _metadata and _data_thumbnail.parquet\n",
    "paths.get_parquet_metadata_pointer(dia_object_lc_path).unlink(missing_ok=True)\n",
    "paths.get_data_thumbnail_pointer(dia_object_lc_path).unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update partition_info.csv\n",
    "partition_info = PartitionInfo.from_healpix(pixels)\n",
    "partition_info_file = paths.get_partition_info_pointer(dia_object_lc_path)\n",
    "partition_info.write_to_file(partition_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6774103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hats.properties and properties\n",
    "from lsdb.catalog.dataset.dataset import Dataset\n",
    "\n",
    "old_properties = dia_object_lc.hc_structure.catalog_info\n",
    "\n",
    "new_props = dict(\n",
    "    Dataset.new_provenance_properties(dia_object_lc_path),\n",
    "    total_rows=old_properties.total_rows + int(np.sum(counts)),\n",
    "    hats_order=partition_info.get_highest_order(),\n",
    "    hats_max_rows=None,\n",
    "    skymap_order=None,\n",
    "    skymap_alt_orders=None,\n",
    "    moc_sky_fraction=f\"{partition_info.calculate_fractional_coverage():0.5f}\",\n",
    "    # There is an issue with setting the default columns\n",
    "    default_columns=old_properties.default_columns,\n",
    "    npix_suffix=\"/\",\n",
    ")\n",
    "dia_object_lc.hc_structure.catalog_info.copy_and_update(**new_props).to_properties_file(dia_object_lc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
