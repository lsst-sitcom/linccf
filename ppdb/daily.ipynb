{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7544b18c",
   "metadata": {},
   "source": [
    "## Daily increments\n",
    "\n",
    "- Keep the existing partitioning schema.\n",
    "\n",
    "- Add a new parquet file to each pixel.\n",
    "\n",
    "  - Account for the object rows of latest validity start only.\n",
    "  \n",
    "  - Find out where each new object lies.\n",
    "\n",
    "  - Generate the margin caches for sources and forced sources.\n",
    "  \n",
    "  - Nest the sources and forced sources in their respective objects.\n",
    "\n",
    "  - Copy the new daily parquet files into their respective pixel directories.\n",
    "\n",
    "- Delete the outdated _metadata file.\n",
    "\n",
    "- Update hats.properties, partition_info.csv and skymaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a64414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hats\n",
    "import lsdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422543f",
   "metadata": {},
   "source": [
    "Specify the paths to the existing and new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99730c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# path to the new PPDB data\n",
    "PPDB_DIR = Path(\"/sdf/scratch/rubin/ppdb/data/lsstcam\")\n",
    "\n",
    "# path to the pre-existing catalog\n",
    "dia_object_lc_path = Path(\"dia_object_lc\")\n",
    "\n",
    "# temporary directory\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "print(f\"Intermediate directory: {tmp_dir.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d7ad9",
   "metadata": {},
   "source": [
    "Initialize a Dask Client for parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\n",
    "    n_workers=16, threads_per_worker=1, local_directory=tmp_dir.name, memory_limit=\"8GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e91d12",
   "metadata": {},
   "source": [
    "Load existing nested catalog with `Npix` directory partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The catalog has leaf pixel directories\n",
    "dia_object_lc = lsdb.open_catalog(dia_object_lc_path)\n",
    "existing_pixels = dia_object_lc.get_healpix_pixels()\n",
    "existing_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest pixel order for the existing catalog\n",
    "mapping_order = dia_object_lc.hc_structure.catalog_info.skymap_order\n",
    "print(f\"mapping_order = {mapping_order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f918a33",
   "metadata": {},
   "source": [
    "Get the new increments for objects, sources and forced sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ae367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(dataset_type, start=10, end=20):\n",
    "    \"\"\"Return the latest parquet file for a given dataset type.\"\"\"\n",
    "    dataset_name = \"\".join(word.capitalize() for word in dataset_type.split(\"_\"))\n",
    "    files = sorted(PPDB_DIR.rglob(f\"{dataset_name}.parquet\"))\n",
    "    return files[start:end]\n",
    "\n",
    "\n",
    "new_object_files = get_paths(\"dia_object\")\n",
    "new_source_files = get_paths(\"dia_source\")\n",
    "new_forced_source_files = get_paths(\"dia_forced_source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cd9ef",
   "metadata": {},
   "source": [
    "When importing new data, we want to keep the existing partitioning structure intact.\n",
    "\n",
    "We'll add new pixels if the alerts include data that lies outside the current pixel coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6db0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hats_import import pipeline_with_client\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "\n",
    "\n",
    "def import_dataset(dataset_type, input_file_list, catalog_type):\n",
    "    args = ImportArguments(\n",
    "        output_path=tmp_dir.name,\n",
    "        output_artifact_name=dataset_type,\n",
    "        input_file_list=input_file_list,\n",
    "        file_reader=\"parquet\",\n",
    "        ra_column=\"ra\",\n",
    "        dec_column=\"dec\",\n",
    "        catalog_type=catalog_type,\n",
    "        pixel_threshold=5_000_000,\n",
    "        highest_healpix_order=mapping_order,\n",
    "        simple_progress_bar=True,\n",
    "        existing_pixels=[(p.order, p.pixel) for p in existing_pixels],\n",
    "        resume=False,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10273c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset(\"dia_object\", new_object_files, catalog_type=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset(\"dia_source\", new_source_files, catalog_type=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27afdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset(\"dia_forced_source\", new_forced_source_files, catalog_type=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e847419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check that the new pixels are a subset of the existing pixels\"\"\"\n",
    "\n",
    "for cat_name in [\"dia_object\", \"dia_source\", \"dia_forced_source\"]:\n",
    "    cat = hats.read_hats(f\"{tmp_dir.name}/{cat_name}\")\n",
    "    new_pixels = set(cat.get_healpix_pixels())\n",
    "    assert set(existing_pixels).issubset(new_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08f31d",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "Same post-processing steps that were applied to the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocess import postprocess_catalog\n",
    "\n",
    "flux_col_prefixes = [f\"{band}_scienceFluxMean\" for band in list(\"ugrizy\")]\n",
    "postprocess_catalog(\"dia_object\", flux_col_prefixes, tmp_dir.name, client)\n",
    "postprocess_catalog(\"dia_source\", [\"scienceFlux\"], tmp_dir.name, client)\n",
    "postprocess_catalog(\"dia_forced_source\", [\"scienceFlux\"], tmp_dir.name, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cbe41",
   "metadata": {},
   "source": [
    "### Create nested increment\n",
    "\n",
    "Nest sources in objects and sort them by MJD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "\n",
    "\n",
    "def load_sources_with_margin(dataset_type, margin_arcsec=5):\n",
    "    \"\"\"Create source margins for nesting\"\"\"\n",
    "    input_catalog_path = f\"{tmp_dir.name}/{dataset_type}\"\n",
    "    margin_name = f\"{dataset_type}_{margin_arcsec}arcs\"\n",
    "\n",
    "    args = MarginCacheArguments(\n",
    "        input_catalog_path=input_catalog_path,\n",
    "        output_path=tmp_dir.name,\n",
    "        margin_threshold=margin_arcsec,\n",
    "        output_artifact_name=margin_name,\n",
    "        progress_bar=False,\n",
    "        resume=False,\n",
    "    )\n",
    "    pipeline_with_client(args, client)\n",
    "    margin_path = f\"{tmp_dir.name}/{margin_name}\"\n",
    "    return lsdb.read_hats(input_catalog_path, margin_cache=margin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load object catalog\n",
    "dia_object = lsdb.read_hats(f\"{tmp_dir.name}/dia_object\")\n",
    "\n",
    "# Load the source catalogs with margins\n",
    "dia_source = load_sources_with_margin(\"dia_source\")\n",
    "dia_forced_source = load_sources_with_margin(\"dia_forced_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7234b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest import nest_sources\n",
    "\n",
    "new_dia_object_lc = nest_sources(dia_object, dia_source, dia_forced_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61f536",
   "metadata": {},
   "source": [
    "### Update existing catalog\n",
    "\n",
    "Write the partitions with the new data to the existing catalog and update the relevant metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from increment import write_partitions, update_skymaps, update_metadata\n",
    "\n",
    "new_pixels, new_counts, new_histograms = write_partitions(new_dia_object_lc, dia_object_lc_path, mapping_order)\n",
    "update_skymaps(dia_object_lc, new_histograms, mapping_order)\n",
    "update_metadata(dia_object_lc, new_pixels, new_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc2890",
   "metadata": {},
   "source": [
    "Close the Dask Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
