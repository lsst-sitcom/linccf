{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4703fba8",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "\n",
    "- Keep rows of latest validity start, for each object.\n",
    "- Add magnitude science columns.\n",
    "- Cast non-(positional/time) columns to float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23721fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import hats\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import tempfile\n",
    "\n",
    "from dask.distributed import as_completed, Client\n",
    "from hats.catalog import PartitionInfo\n",
    "from hats.io import paths\n",
    "from hats.io.parquet_metadata import write_parquet_metadata\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d237881",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_dir = Path(\"/sdf/scratch/rubin/ppdb/data/lsstcam\")\n",
    "hats_dir = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/hats/PPDB_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = Path(tmp_path.name)\n",
    "client = Client(n_workers=16, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08326b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_latest_validity(table):\n",
    "    \"\"\"Select rows with the latest validityStart for each object.\"\"\"\n",
    "    return table.sort_values(\"validityStart\").drop_duplicates(\"diaObjectId\", keep=\"last\")\n",
    "\n",
    "\n",
    "def append_mag_and_magerr(table, flux_cols):\n",
    "    \"\"\"Calculate magnitudes and their errors for flux columns.\"\"\"\n",
    "    mag_cols = {}\n",
    "\n",
    "    for flux_col in flux_cols:\n",
    "        flux_col_err = f\"{flux_col}Err\"\n",
    "        mag_col = flux_col.replace(\"Flux\", \"Mag\")\n",
    "        mag_col_err = f\"{mag_col}Err\"\n",
    "        \n",
    "        flux = table[flux_col]\n",
    "        mag = u.nJy.to(u.ABmag, flux)\n",
    "        mag_cols[mag_col] = mag\n",
    "\n",
    "        flux_err = table[flux_col_err]\n",
    "        upper_mag = u.nJy.to(u.ABmag, flux + flux_err)\n",
    "        lower_mag = u.nJy.to(u.ABmag, flux - flux_err)\n",
    "        magErr = -(upper_mag - lower_mag) / 2\n",
    "        mag_cols[mag_col_err] = magErr\n",
    "\n",
    "    mag_table = pd.DataFrame(\n",
    "        mag_cols, dtype=pd.ArrowDtype(pa.float32()), index=table.index\n",
    "    )\n",
    "    return pd.concat([table, mag_table], axis=1)\n",
    "\n",
    "def cast_columns_float32(table):\n",
    "    \"\"\"Cast non-(positional/time) columns to single-precision\"\"\"\n",
    "    position_time_cols = [\n",
    "        \"ra\",\n",
    "        \"dec\",\n",
    "        \"raErr\",\n",
    "        \"decErr\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"xErr\",\n",
    "        \"yErr\",\n",
    "        \"midpointMjdTai\",\n",
    "        \"radecMjdTai\",\n",
    "    ]\n",
    "    columns_to_cast = [\n",
    "        field\n",
    "        for (field, type) in table.dtypes.items()\n",
    "        if field not in position_time_cols and type == pd.ArrowDtype(pa.float64())\n",
    "    ]\n",
    "    dtype_map = {col: pd.ArrowDtype(pa.float32()) for col in columns_to_cast}\n",
    "    return table.astype(dtype_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1213c",
   "metadata": {},
   "source": [
    "Let's add code to parallelize these operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_catalog(catalog_name, flux_col_prefixes):\n",
    "    catalog_dir = hats_dir / catalog_name\n",
    "    catalog = hats.read_hats(catalog_dir)\n",
    "    futures = [] \n",
    "    for target_pixel in catalog.get_healpix_pixels():\n",
    "        futures.append(\n",
    "            client.submit(\n",
    "                process_partition,\n",
    "                catalog_dir=catalog_dir,\n",
    "                target_pixel=target_pixel,\n",
    "                flux_col_prefixes=flux_col_prefixes,\n",
    "            )\n",
    "        )\n",
    "    for future in tqdm(as_completed(futures), desc=catalog_name, total=len(futures)):\n",
    "        if future.status == \"error\":\n",
    "            raise future.exception()\n",
    "    rewrite_catalog_metadata(catalog)\n",
    "\n",
    "\n",
    "def process_partition(catalog_dir, target_pixel, flux_col_prefixes):\n",
    "    \"\"\"Apply post-processing steps to each individual partition\"\"\"\n",
    "    file_path = hats.io.pixel_catalog_file(catalog_dir, target_pixel)\n",
    "    table = pd.read_parquet(file_path, dtype_backend=\"pyarrow\")\n",
    "    if \"validityStart\" in table.columns:\n",
    "        table = select_by_latest_validity(table)\n",
    "    if len(flux_col_prefixes) > 0:\n",
    "        table = append_mag_and_magerr(table, flux_col_prefixes)\n",
    "    table = cast_columns_float32(table)\n",
    "    final_table = pa.Table.from_pandas(table, preserve_index=False).replace_schema_metadata()\n",
    "    pq.write_table(final_table, file_path.path)\n",
    "\n",
    "\n",
    "def rewrite_catalog_metadata(catalog):\n",
    "    \"\"\"Update catalog metadata after processing the leaf parquet files\"\"\"\n",
    "    destination_path = hats_dir / catalog.catalog_name\n",
    "    parquet_rows = write_parquet_metadata(destination_path)\n",
    "    # Read partition info from _metadata and write to partition_info.csv\n",
    "    partition_info = PartitionInfo.read_from_dir(destination_path)\n",
    "    partition_info_file = paths.get_partition_info_pointer(destination_path)\n",
    "    partition_info.write_to_file(partition_info_file)\n",
    "    now = datetime.now(tz=timezone.utc)\n",
    "    catalog.catalog_info.copy_and_update(\n",
    "        total_rows=parquet_rows, hats_creation_date=now.strftime(\"%Y-%m-%dT%H:%M%Z\")\n",
    "    ).to_properties_file(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a8c3a",
   "metadata": {},
   "source": [
    "For DIA objects, calculate the mean magnitudes per band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe339",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_col_prefixes = [f\"{band}_scienceFluxMean\" for band in list(\"ugrizy\")]\n",
    "postprocess_catalog(\"dia_object\", flux_col_prefixes=flux_col_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e1a23",
   "metadata": {},
   "source": [
    "For DIA source and forced source, calculate their science magnitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74af33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_catalog(\"dia_source\", flux_col_prefixes=[\"scienceFlux\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02489800",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_catalog(\"dia_forced_source\", flux_col_prefixes=[\"scienceFlux\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
