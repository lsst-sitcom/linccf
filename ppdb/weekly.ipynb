{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77202e6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Weekly aggregation\n",
    "\n",
    "- Aggregate the daily parquets of each pixel into the past.parquet file.\n",
    "\n",
    "    - Merge duplicate object rows.\n",
    "\n",
    "    - Keep the latest values for each conflicting object columns.\n",
    "\n",
    "    - Merge the light curve nested columns.\n",
    "\n",
    "- Repartition each pixel according to a pre-defined threshold argument.\n",
    "\n",
    "- Regenerate _metadata file.\n",
    "\n",
    "- Regenerate collection with margin cache and index catalog (from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81594a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from dask.distributed import Client\n",
    "from pathlib import Path\n",
    "\n",
    "hats_dir = Path(\"ppdb\")\n",
    "\n",
    "# Initializing the Dask Client to parallelize operations\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "print(f\"Intermediate directory: {tmp_dir.name}\")\n",
    "client = Client(\n",
    "    n_workers=16, threads_per_worker=1, local_directory=tmp_dir.name, memory_limit=\"8GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af49db3",
   "metadata": {},
   "source": [
    "- Objects near pixel borders might have moved to one of their neighboring partitions. \n",
    "- To merge object alerts accurately we need to generate margins for the daily catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hats_import import pipeline_with_client\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "\n",
    "args = MarginCacheArguments(\n",
    "    input_catalog_path=hats_dir / \"dia_object_lc\",\n",
    "    output_path=hats_dir,\n",
    "    margin_threshold=10,  # 10 arcsec\n",
    "    output_artifact_name=\"dia_object_lc_10arcs\",\n",
    "    simple_progress_bar=True,\n",
    "    resume=False,\n",
    ")\n",
    "pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42446bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_object_lc = lsdb.open_catalog(\n",
    "    hats_dir / \"dia_object_lc\", margin_cache=hats_dir / \"dia_object_lc_10arcs\"\n",
    ")\n",
    "dia_object_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d09bd",
   "metadata": {},
   "source": [
    "We should now merge each partition with their margins on \"diaObjectId\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99841bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hats as hc\n",
    "import pandas as pd\n",
    "\n",
    "from lsdb import Catalog\n",
    "from lsdb.dask.merge_catalog_functions import (\n",
    "    align_and_apply,\n",
    "    align_catalogs,\n",
    "    construct_catalog_args,\n",
    "    get_healpix_pixels_from_alignment,\n",
    "    filter_by_spatial_index_to_pixel,\n",
    ")\n",
    "\n",
    "\n",
    "def merge_object_data(dia_object_lc):\n",
    "    alignment = align_catalogs(dia_object_lc, dia_object_lc)\n",
    "    _, pixels = get_healpix_pixels_from_alignment(alignment)\n",
    "    joined_partitions = align_and_apply(\n",
    "        [(dia_object_lc, pixels), (dia_object_lc.margin, pixels)],\n",
    "        perform_join_on,\n",
    "    )\n",
    "    ddf, ddf_map, alignment = construct_catalog_args(\n",
    "        joined_partitions,\n",
    "        dia_object_lc._ddf._meta,\n",
    "        alignment,\n",
    "    )\n",
    "    hc_catalog = hc.catalog.Catalog(\n",
    "        dia_object_lc.hc_structure.catalog_info,\n",
    "        alignment.pixel_tree,\n",
    "        schema=dia_object_lc.original_schema,  # the schema is the same\n",
    "        moc=alignment.moc,\n",
    "    )\n",
    "    return Catalog(ddf, ddf_map, hc_catalog)\n",
    "\n",
    "\n",
    "def perform_join_on(df, margin, df_pixel, *args):\n",
    "    original_cols = list(df.columns)\n",
    "\n",
    "    # 1. Join df with margin\n",
    "    final_df = pd.concat([df, margin])\n",
    "\n",
    "    # 2. Order each object by validityStart\n",
    "    final_df = final_df.sort_values([\"diaObjectId\", \"validityStart\"], ascending=[True,False])\n",
    "\n",
    "    # 3. Get the sources for all the objects\n",
    "    final_df[\"diaSource.diaObjectId\"] = final_df[\"diaObjectId\"]\n",
    "    final_df[\"diaForcedSource.diaObjectId\"] = final_df[\"diaObjectId\"]\n",
    "    sources = final_df[\"diaSource\"].explode().sort_values([\"midpointMjdTai\"])\n",
    "    fsources = final_df[\"diaForcedSource\"].explode().sort_values([\"midpointMjdTai\"])\n",
    "\n",
    "    # 4. Grab the latest row per object\n",
    "    _, latest_indices = np.unique(final_df[\"diaObjectId\"], return_index=True)\n",
    "    final_df = final_df.iloc[latest_indices]\n",
    "\n",
    "    # 5. Drop the sources and join them again\n",
    "    final_df = final_df.drop(columns=[\"diaSource\", \"diaForcedSource\"])\n",
    "    final_df = final_df.join_nested(sources, \"diaSource\", on=\"diaObjectId\")\n",
    "    final_df = final_df.join_nested(fsources, \"diaForcedSource\", on=\"diaObjectId\")\n",
    "\n",
    "    # 6. Filter out points outside of the pixel (that are therefore in margin)\n",
    "    final_df = filter_by_spatial_index_to_pixel(final_df, df_pixel.order, df_pixel.pixel)\n",
    "\n",
    "    # 7. Make sure columns keep the same order\n",
    "    return final_df[original_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb78591",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lc = merge_object_data(dia_object_lc)\n",
    "merged_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check on the expected IDs:\n",
    "actual_ids = np.unique(merged_lc[\"diaObjectId\"].compute())\n",
    "expected_ids = np.unique(dia_object_lc[\"diaObjectId\"].compute())\n",
    "np.testing.assert_array_equal(actual_ids, expected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366cd6b",
   "metadata": {},
   "source": [
    "Then we need to write this catalog to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f67c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lc.write_catalog(f\"{tmp_dir.name}/dia_object_lc\", as_collection=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6744454",
   "metadata": {},
   "source": [
    "And use `hats-import` to reimport with different threshold, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hats_import.catalog.run_import as catalog_runner\n",
    "from hats_import import ImportArguments\n",
    "\n",
    "catalog_args = ImportArguments.reimport_from_hats(\n",
    "    path=f\"{tmp_dir.name}/dia_object_lc\",\n",
    "    output_dir=hats_dir/\"dia_object_collection\",\n",
    "    output_artifact_name=\"dia_object_lc\",\n",
    "    pixel_threshold=1_000_000, # use byte_threshold when it's available?\n",
    "    skymap_alt_orders=[2,4,6],\n",
    "    npix_suffix=\"/\", # make sure it's stored in Npix dirs\n",
    "    addl_hats_properties={\"hats_cols_default\": dia_object_lc.hc_structure.catalog_info.default_columns},\n",
    ")\n",
    "catalog_runner.run(catalog_args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
